{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook réalisé par Aristide LALOUX, Hugo QUENIAT, Mohamed Ali SRIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "- S. Kumar. Reinforcement learning code for cartpole system. https://github.com/swagatk/RL-Projects-SK.git, 2020\n",
    "- S. Kumar. Balancing a CartPole System with Reinforcement Learning - A Tutorial, https://arxiv.org/pdf/2006.04938.pdf?fbclid=IwAR1HWA-WRWQL3BwKAmM7vKq-DePOkjkalVAqFvL61AtlEGigytR6xwlRVQs, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Création de l'environnement\n",
    "\n",
    "- Gym est une bibliothèque python qui contient de nombreux modèles pour l'apprentissage par renforcement : nous allons utiliser un exemple simple, celui du CartPole (voir sa description dans le code source https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "- Il est modélisé par un vecteur de 4 dimensions : $ \\{ x , \\dot x ,\\theta ,\\dot \\theta \\}$ respectivement : la position du chariot,  la vitesse du chariot, l'angle de la barre et la vitesse angulaire de la barre.\n",
    "\n",
    "- Il y a deux actions possible : mouvement droit ou gauche du chariot\n",
    "\n",
    "- Pour s'entraîner l'algo effectue des épisodes (ici maximum 1000)\n",
    "\n",
    "- À chaque épisode on prend maximum 200 décisions\n",
    "\n",
    "- La récompense est calculée comme étant le nombre de tours dans un épisode où la barre est resté stable.\n",
    "\n",
    "- La stabilité est définie par ses 4 critères :\n",
    "\n",
    "- - Ne pas dépasser $ \\theta $  +- 12° \n",
    "\n",
    "- - Ne pas dépasser  $ x $ 2.4 cm\n",
    "\n",
    "- À chaque décision le reward est incrémenté de 1, le Reward de l'épisode correspond donc au nombre d'actions qui ont été effectuées tout en conservant la barre stable\n",
    "\n",
    "- Si le Reward des 100 derniers épisodes a une moyenne supérieur à 195, on considère qu'on est assez proche du Q optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importer l'environnement de Gym\n",
    "\n",
    "env= gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-defined parameters\n",
    "max_episodes = 2000\n",
    "max_time_steps = 250\n",
    "streak_to_end = 120\n",
    "solved_time = 199\n",
    "discount = 0.99\n",
    "no_streaks = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regardons ensembles les valeurs possibles des états du système (soit les plages d'existence de $ x , \\dot x ,\\theta ,\\dot \\theta$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-4.8, 4.8), (-3.4028235e+38, 3.4028235e+38), (-0.41887903, 0.41887903), (-3.4028235e+38, 3.4028235e+38)]\n",
      "4\n",
      "(4, 2)\n",
      "-4.8\n"
     ]
    }
   ],
   "source": [
    "state_value_bounds = list(zip(env.observation_space.low, \n",
    "                              env.observation_space.high)) \n",
    "\n",
    "print(state_value_bounds)\n",
    "print(len(state_value_bounds))\n",
    "print(np.shape(state_value_bounds))\n",
    "print(state_value_bounds[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Étant donné qu'il sera nécessaire de discrétiser (voir cellules suivantes) afin d'avoir un ensemble fini d'état, on va restreindre les vitesses à de plus petites valeurs (au lieu de + - l'infini comme ci dessus) pour pouvoir bien subdiviser notre espace sans avoir beaucoup trop d'états. (Cette subdivision s'effectue de façon à ce que les vitesses supérieures, en valeur absolue, à un certain minimum correspondent à un unique état. En effet, lorsque la vitesse est trop élevée, il devient presque impossible de conserver l'équilibre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_value_bounds[1] = (-0.5, 0.5)\n",
    "state_value_bounds[3] = (-math.radians(50), math.radians(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le tuple no_actions représente le nombre d'actions possibles à chaque instant. Dans notre cas, nous en choisissons 2 : le chariot peut aller soit à droite soit à gauche à chaque fois.\n",
    "\n",
    "- Le tuple no_buckets représente le nombre de subdivision de l'espace continu des vecteurs-états. (En prenant 1 pour valeur de position et vitesse on veut dire que toutes les valeurs sont équivalentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_actions = env.action_space.n\n",
    "no_buckets = (1,1,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de discretisation\n",
    "\n",
    "def bucketize_state_value(state_value):\n",
    "    ''' Discretizes continuous values into fixed buckets'''\n",
    "    #print('len(state_value):', len(state_value))\n",
    "    bucket_indices = []\n",
    "    for i in range(len(state_value)):\n",
    "        if state_value[i] <= state_value_bounds[i][0]:   # violates lower bound\n",
    "            bucket_index = 0\n",
    "        elif state_value[i] >= state_value_bounds[i][1]: # violates upper bound\n",
    "            bucket_index = no_buckets[i] - 1  # put in the last bucket\n",
    "        else:\n",
    "            bound_width = state_value_bounds[i][1] - state_value_bounds[i][0]\n",
    "            offset = (no_buckets[i]-1) * state_value_bounds[i][0] / bound_width\n",
    "            scaling = (no_buckets[i]-1) / bound_width\n",
    "            bucket_index = int(round(scaling*state_value[i]-offset))\n",
    "\n",
    "        bucket_indices.append(bucket_index)\n",
    "    return(tuple(bucket_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On initialise arbitrairement Q, c'est une matrice des états $ \\times $ actions. On a $ 1 \\times 2 \\times 6 \\times 3 $ états, et 2 actions. L'élément Q[état][action] est l'espérence conditionnelle des récompenses des états futurs sachant qu'on a pris la décision de faire action quand on était à l'état état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_table = np.zeros(no_buckets + (no_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Appliquer l'algorithme de Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La méthode Epsilon-Greedy (Voir La biblio)"
=======
    "- Il ne reste qu'à appliquer l'algorithme de Q-Learning avec la méthode ${\\epsilon}$-greedy. (cf bibliographie)"
>>>>>>> 11d8a12bf893e30ae1f24fb1cf86831ac0c39e83
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action - explore vs exploit\n",
    "# epsilon-greedy method\n",
    "\n",
    "min_explore_rate = 0.1\n",
    "min_learning_rate = 0.07\n",
    "\n",
    "def select_action(state_value, explore_rate):\n",
    "    if random.random() < explore_rate: \n",
    "        action = env.action_space.sample()    # explore\n",
    "    else:\n",
    "        action = np.argmax(q_value_table[state_value])  # exploit\n",
    "    return action\n",
    "\n",
    "def select_explore_rate(x):\n",
    "    # change the exploration rate over time.\n",
    "    return max(min_learning_rate, min(1.0, 1.0 - math.log10((x+1)/25)))\n",
    "\n",
    "def select_learning_rate(x):\n",
    "    # Change learning rate over time\n",
    "    return max(min_learning_rate, min(1.0, 1.0 - math.log10((x+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 15 time steps\n",
      "Episode 100 finished after 146 time steps\n",
      "Episode 200 finished after 200 time steps\n",
      "CartPole problem is solved after 273 episodes.\n",
      "[[[[[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]\n",
      "\n",
      "   [[35.21837843 38.25491142]\n",
      "    [24.06811218 34.44635847]\n",
      "    [60.15197869  0.        ]]\n",
      "\n",
      "   [[99.87049482 99.29793412]\n",
      "    [99.87398191 99.85950241]\n",
      "    [99.72916418 99.87363773]]\n",
      "\n",
      "   [[99.87304502 99.78052018]\n",
      "    [99.85579863 99.87397068]\n",
      "    [99.51423829 99.8711305 ]]\n",
      "\n",
      "   [[37.24766829  0.        ]\n",
      "    [54.37973169 53.48018038]\n",
      "    [52.8728625  54.34602758]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]]]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.zeros(no_buckets + (no_actions,))\n",
    "\n",
    "_DEBUG = False\n",
    "frames = []\n",
    "reward_per_episode = []\n",
    "time_per_episode = []\n",
    "avgtime_per_episode = []\n",
    "learning_rate_per_episode = []\n",
    "explore_rate_per_episode = []\n",
    "\n",
    "Qstar = None\n",
    "evolutionQeps = []\n",
    "\n",
    "\n",
    "\n",
    "# train the system\n",
    "totaltime = 0\n",
    "for episode_no in range(max_episodes):\n",
    "\n",
    "    explore_rate = select_explore_rate(episode_no)\n",
    "    learning_rate = select_learning_rate(episode_no)\n",
    "\n",
    "    learning_rate_per_episode.append(learning_rate)\n",
    "    explore_rate_per_episode.append(explore_rate)\n",
    "\n",
    "    # reset the environment while starting a new episode\n",
    "    observation = env.reset()\n",
    "    \n",
    "    start_state_value = bucketize_state_value(observation)\n",
    "    previous_state_value = start_state_value\n",
    "\n",
    "    #print(start_state_value)\n",
    "    #time.sleep(10)\n",
    "\n",
    "\n",
    "    done = False \n",
    "    time_step = 0\n",
    "\n",
    "    while not done:  \n",
    "        #env.render()\n",
    "        action = select_action(previous_state_value, explore_rate)\n",
    "        observation, reward_gain, done, info = env.step(action)\n",
    "        #time.sleep(1) voir ce qui se passe à chaque itération\n",
    "        state_value = bucketize_state_value(observation)\n",
    "        best_q_value = np.max(q_value_table[state_value])\n",
    "\n",
    "        #update q_value_table\n",
    "        q_value_table[previous_state_value][action] += learning_rate * (\n",
    "            reward_gain + discount * best_q_value - \n",
    "            q_value_table[previous_state_value][action])\n",
    "\n",
    "        #print(q_value_table)\n",
    "\n",
    "        evolutionQeps.append(max(q_value_table[0][0][3][1]))\n",
    "\n",
    "\n",
    "\n",
    "        previous_state_value = state_value\n",
    "\n",
    "        #Enlever le if si vous voulez plus de détails \n",
    "        \n",
    "        if episode_no % 100 == 0 and _DEBUG == True:\n",
    "            print('Episode number: {}'.format(episode_no))\n",
    "            print('Time step: {}'.format(time_step))\n",
    "            print('Previous State Value: {}'.format(previous_state_value))\n",
    "            print('Selected Action: {}'.format(action))\n",
    "            print('Current State: {}'.format(str(state_value)))\n",
    "            print('Reward Obtained: {}'.format(reward_gain))\n",
    "            print('Best Q Value: {}'.format(best_q_value))\n",
    "            print('Learning rate: {}'.format(learning_rate))\n",
    "            print('Explore rate: {}'.format(explore_rate))\n",
    "\n",
    "        \n",
    "\n",
    "        time_step += 1\n",
    "        # while loop ends here\n",
    "\n",
    "    if time_step >= solved_time:\n",
    "        no_streaks += 1\n",
    "    else:\n",
    "        no_streaks = 0\n",
    "\n",
    "    if no_streaks > streak_to_end:\n",
    "        print('CartPole problem is solved after {} episodes.'.format(episode_no))\n",
    "        break\n",
    "\n",
    "    # data log\n",
    "    if episode_no % 100 == 0:  \n",
    "        print('Episode {} finished after {} time steps'.format(episode_no, time_step))\n",
    "    time_per_episode.append(time_step)\n",
    "    totaltime += time_step\n",
    "    avgtime_per_episode.append(totaltime/(episode_no+1))\n",
    "    # episode loop ends here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "Qeps = q_value_table\n",
    "\n",
    "print(Qeps)\n",
    "\n",
    "#print(evolutionQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate et exploration rate constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester notre modèle entrainé\n",
    "\n",
    "- On va suivre la meilleur décision selon notre algo et voir si ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_trained(state_value, Q):\n",
    "    \n",
    "    action = np.argmax(Q[state_value])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 finished after 9 time steps\n",
      "Test 1 finished after 10 time steps\n",
      "Test 2 finished after 10 time steps\n",
      "Test 3 finished after 10 time steps\n",
      "Test 4 finished after 10 time steps\n",
      "Test 5 finished after 10 time steps\n",
      "Test 6 finished after 9 time steps\n",
      "Test 7 finished after 9 time steps\n",
      "Test 8 finished after 11 time steps\n",
      "Test 9 finished after 9 time steps\n",
      "Test 10 finished after 9 time steps\n",
      "Test 11 finished after 9 time steps\n",
      "Test 12 finished after 9 time steps\n",
      "Test 13 finished after 10 time steps\n",
      "Test 14 finished after 8 time steps\n",
      "Test 15 finished after 10 time steps\n",
      "Test 16 finished after 9 time steps\n",
      "Test 17 finished after 9 time steps\n",
      "Test 18 finished after 11 time steps\n",
      "Test 19 finished after 10 time steps\n"
     ]
    }
   ],
   "source": [
    "Qaleatoire = np.zeros(no_buckets + (no_actions,))\n",
    "\n",
    "for i in range (20) :\n",
    "\n",
    "    for episode_no in range(10):\n",
    "        \n",
    "        observation = env.reset()\n",
    "        start_state_value = bucketize_state_value(observation)\n",
    "        previous_state_value = start_state_value\n",
    "        done = False \n",
    "        time_step = 0\n",
    "    \n",
    "        while not done:  \n",
    "                \n",
    "            env.render()\n",
    "\n",
    "            action = select_action_trained(previous_state_value, Qaleatoire)\n",
    "            observation, reward_gain, done, info = env.step(action)\n",
    "            #time.sleep(1) voir ce qui se passe à chaque itération\n",
    "            state_value = bucketize_state_value(observation)\n",
    "            previous_state_value = state_value\n",
    "\n",
    "                    #Enlever le if si vous voulez plus de détails \n",
    "            \n",
    "            if episode_no % 100 == 0 and _DEBUG == True:\n",
    "                print('Episode number: {}'.format(episode_no))\n",
    "                print('Time step: {}'.format(time_step))\n",
    "                print('Previous State Value: {}'.format(previous_state_value))\n",
    "                print('Selected Action: {}'.format(action))\n",
    "                print('Current State: {}'.format(str(state_value)))\n",
    "                print('Reward Obtained: {}'.format(reward_gain))\n",
    "                print('Best Q Value: {}'.format(best_q_value))\n",
    "                print('Learning rate: {}'.format(learning_rate))\n",
    "                print('Explore rate: {}'.format(explore_rate))\n",
    "\n",
    "            \n",
    "\n",
    "            time_step += 1\n",
    "            # while loop ends here\n",
    "\n",
    "        if time_step >= solved_time:\n",
    "            no_streaks += 1\n",
    "        else:\n",
    "            no_streaks = 0\n",
    "\n",
    "\n",
    "        # data log\n",
    "        if episode_no % 100 == 0:  \n",
    "            print('Test {} finished after {} time steps'.format(i, time_step))\n",
    "        time_per_episode.append(time_step)\n",
    "        totaltime += time_step\n",
    "        avgtime_per_episode.append(totaltime/(episode_no+1))\n",
    "        # episode loop ends here\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étudier l'effet de certains paramètre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le paramètre $ \\alpha $\n",
    "- On veut voir l'effet du Learning rate sur la variation de $ Q(s,a) $ pour un état donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(evolutionQeps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
