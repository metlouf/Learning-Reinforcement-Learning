{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook réalisé par Mohamed Ali SRIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "- S. Kumar. Reinforcement learning code for cartpole system. https://github.com/swagatk/RL-Projects-SK.git, 2020\n",
    "- S. Kumar. Balancing a CartPole System with Reinforcement Learning - A Tutorial, https://arxiv.org/pdf/2006.04938.pdf?fbclid=IwAR1HWA-WRWQL3BwKAmM7vKq-DePOkjkalVAqFvL61AtlEGigytR6xwlRVQs, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Création de l'environnement\n",
    "\n",
    "- Gym est une bibliothèque python qui contient plein de modèle pour l'apprentissage par renforcement : nous allons utilisé un exemple simple celui du CartPole (voir sa description dans le code source https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "- Il est modélisé par un vecteur de 4 dimensions : $ \\{ x , \\dot x ,\\theta ,\\dot \\theta \\}$ respectivement : la position du chariot,  la vitesse du chariot, l'angle de la barre et la vitesse angulaire de la barre.\n",
    "\n",
    "- Il y a deux actions possible : mouvement droit ou gauche du chariot\n",
    "\n",
    "- Pour s'entraîner l'algo effectue des épisodes (ici maximum 1000)\n",
    "\n",
    "- À chaque épisode on prend maximum 200 décisions\n",
    "\n",
    "- La récompense et calculé comme étant le nombre de tour dans un épisode où la barre est resté stable.\n",
    "\n",
    "- La stabilité est définie par ses 4 critères :\n",
    "\n",
    "- - Ne pas dépasser $ \\theta $  +- 12° \n",
    "\n",
    "- - Ne pas dépasser  $ x $ 2.4 cm\n",
    "\n",
    "- À chaque décision ou incrémente le Reward de 1, le Reward de l'épisode est donc combien il a gardé la barre stable\n",
    "\n",
    "- Si le Reward des 100 derniers épisodes a une moyenne supérieur à 195, on considère qu'on est assez proche du Q optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importer l'environnement de Gym\n",
    "\n",
    "env= gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-defined parameters\n",
    "max_episodes = 20000\n",
    "max_time_steps = 250\n",
    "streak_to_end = 120\n",
    "solved_time = 199\n",
    "discount = 0.99\n",
    "no_streaks = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regardons ensembles les valeurs possibles des états du système (soit les plages d'existence de $ x , \\dot x ,\\theta ,\\dot \\theta$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-4.8, 4.8), (-3.4028235e+38, 3.4028235e+38), (-0.41887903, 0.41887903), (-3.4028235e+38, 3.4028235e+38)]\n",
      "4\n",
      "(4, 2)\n",
      "-4.8\n"
     ]
    }
   ],
   "source": [
    "state_value_bounds = list(zip(env.observation_space.low, \n",
    "                              env.observation_space.high)) \n",
    "\n",
    "print(state_value_bounds)\n",
    "print(len(state_value_bounds))\n",
    "print(np.shape(state_value_bounds))\n",
    "print(state_value_bounds[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Étant donné qu'on aura besoin de discrétisé (voir cellules suivantes) pour avoir un ensemble fini d'état, on va restreindre les vitesses à de plus petites valeurs (au lieu de + - l'infinie comme présent) pour pouvoir bien subdiviser notre espace sans avoir beaucoup trop d'états. (La manière dont on va le faire va faire en sorte que en dehors de ces vitesses là on considèrera que c'est le même état ça marche plûtot bien (si on a de trop grandes vitesses les chances de rester en équilibre sont pas grande donc intuitivement ça se comprend))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_value_bounds[1] = (-0.5, 0.5)\n",
    "state_value_bounds[3] = (-math.radians(50), math.radians(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le tuple no_actions représente le nombre d'action possible à chaque instant, dans notre cas c'est deux le chariot peut aller soit à droite soit à gauche à chaque fois.\n",
    "\n",
    "- Le tuple no_buckets représente le nombre de subdivision de l'espace continu des vecteurs-états. (En prenant 1 pour valeur de position et vitesse on veut dire que toutes les valeurs sont équivalentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_actions = env.action_space.n\n",
    "no_buckets = (1,1,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de discretisation\n",
    "\n",
    "def bucketize_state_value(state_value):\n",
    "    ''' Discretizes continuous values into fixed buckets'''\n",
    "    #print('len(state_value):', len(state_value))\n",
    "    bucket_indices = []\n",
    "    for i in range(len(state_value)):\n",
    "        if state_value[i] <= state_value_bounds[i][0]:   # violates lower bound\n",
    "            bucket_index = 0\n",
    "        elif state_value[i] >= state_value_bounds[i][1]: # violates upper bound\n",
    "            bucket_index = no_buckets[i] - 1  # put in the last bucket\n",
    "        else:\n",
    "            bound_width = state_value_bounds[i][1] - state_value_bounds[i][0]\n",
    "            offset = (no_buckets[i]-1) * state_value_bounds[i][0] / bound_width\n",
    "            scaling = (no_buckets[i]-1) / bound_width\n",
    "            bucket_index = int(round(scaling*state_value[i]-offset))\n",
    "\n",
    "        bucket_indices.append(bucket_index)\n",
    "    return(tuple(bucket_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On initialise arbitrairement Q, c'est une matrice des états $ \\times $ actions. On a $ 1 \\times 2 \\times 6 \\times 3 $ états, et 2 actions. L'élément Q[état][action] est l'espérence conditionnelle des récompenses des états futurs sachant qu'on a pris la décision de faire action quand on était à l'état état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_table = np.zeros(no_buckets + (no_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appliquer l'algorithme de Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La méthode Epsilon-Greedy (Voir La biblio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action - explore vs exploit\n",
    "# epsilon-greedy method\n",
    "\n",
    "min_explore_rate = 0.1\n",
    "min_learning_rate = 0.07\n",
    "\n",
    "def select_action(state_value, explore_rate):\n",
    "    if random.random() < explore_rate: \n",
    "        action = env.action_space.sample()    # explore\n",
    "    else:\n",
    "        action = np.argmax(q_value_table[state_value])  # exploit\n",
    "    return action\n",
    "\n",
    "def select_explore_rate(x):\n",
    "    # change the exploration rate over time.\n",
    "    return max(min_learning_rate, min(1.0, 1.0 - math.log10((x+1)/25)))\n",
    "\n",
    "def select_learning_rate(x):\n",
    "    # Change learning rate over time\n",
    "    return max(min_learning_rate, min(1.0, 1.0 - math.log10((x+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 37 time steps\n",
      "Episode 100 finished after 112 time steps\n",
      "Episode 200 finished after 200 time steps\n",
      "CartPole problem is solved after 294 episodes.\n",
      "[[[[[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]\n",
      "\n",
      "   [[46.47580138 48.77517088]\n",
      "    [37.29182269 45.0549841 ]\n",
      "    [ 0.95078198 67.59046517]]\n",
      "\n",
      "   [[99.4464887  97.53380549]\n",
      "    [99.46270445 99.37322387]\n",
      "    [99.11086664 99.46103548]]\n",
      "\n",
      "   [[99.45853951 98.88913635]\n",
      "    [99.38848648 99.46115693]\n",
      "    [97.86852555 99.43758912]]\n",
      "\n",
      "   [[25.57415388  0.        ]\n",
      "    [31.76888278 33.87436817]\n",
      "    [41.12010431 38.3447952 ]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]]]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.zeros(no_buckets + (no_actions,))\n",
    "\n",
    "_DEBUG = False\n",
    "frames = []\n",
    "reward_per_episode = []\n",
    "time_per_episode = []\n",
    "avgtime_per_episode = []\n",
    "learning_rate_per_episode = []\n",
    "explore_rate_per_episode = []\n",
    "\n",
    "Qstar = None\n",
    "evolutionQeps = []\n",
    "\n",
    "\n",
    "\n",
    "# train the system\n",
    "totaltime = 0\n",
    "for episode_no in range(max_episodes):\n",
    "\n",
    "    explore_rate = select_explore_rate(episode_no)\n",
    "    learning_rate = select_learning_rate(episode_no)\n",
    "\n",
    "    learning_rate_per_episode.append(learning_rate)\n",
    "    explore_rate_per_episode.append(explore_rate)\n",
    "\n",
    "    # reset the environment while starting a new episode\n",
    "    observation = env.reset()\n",
    "    \n",
    "    start_state_value = bucketize_state_value(observation)\n",
    "    previous_state_value = start_state_value\n",
    "\n",
    "    #print(start_state_value)\n",
    "    #time.sleep(10)\n",
    "\n",
    "\n",
    "    done = False \n",
    "    time_step = 0\n",
    "\n",
    "    while not done:  \n",
    "        #env.render()\n",
    "        action = select_action(previous_state_value, explore_rate)\n",
    "        observation, reward_gain, done, info = env.step(action)\n",
    "        #time.sleep(1) voir ce qui se passe à chaque itération\n",
    "        state_value = bucketize_state_value(observation)\n",
    "        best_q_value = np.max(q_value_table[state_value])\n",
    "\n",
    "        #update q_value_table\n",
    "        q_value_table[previous_state_value][action] += learning_rate * (\n",
    "            reward_gain + discount * best_q_value - \n",
    "            q_value_table[previous_state_value][action])\n",
    "\n",
    "        #print(q_value_table)\n",
    "\n",
    "        evolutionQeps.append(max(q_value_table[0][0][3][1]))\n",
    "\n",
    "\n",
    "\n",
    "        previous_state_value = state_value\n",
    "\n",
    "        #Enlever le if si vous voulez plus de détails \n",
    "        \n",
    "        if episode_no % 100 == 0 and _DEBUG == True:\n",
    "            print('Episode number: {}'.format(episode_no))\n",
    "            print('Time step: {}'.format(time_step))\n",
    "            print('Previous State Value: {}'.format(previous_state_value))\n",
    "            print('Selected Action: {}'.format(action))\n",
    "            print('Current State: {}'.format(str(state_value)))\n",
    "            print('Reward Obtained: {}'.format(reward_gain))\n",
    "            print('Best Q Value: {}'.format(best_q_value))\n",
    "            print('Learning rate: {}'.format(learning_rate))\n",
    "            print('Explore rate: {}'.format(explore_rate))\n",
    "\n",
    "        \n",
    "\n",
    "        time_step += 1\n",
    "        # while loop ends here\n",
    "\n",
    "    if time_step >= solved_time:\n",
    "        no_streaks += 1\n",
    "    else:\n",
    "        no_streaks = 0\n",
    "\n",
    "    if no_streaks > streak_to_end:\n",
    "        print('CartPole problem is solved after {} episodes.'.format(episode_no))\n",
    "        break\n",
    "\n",
    "    # data log\n",
    "    if episode_no % 100 == 0:  \n",
    "        print('Episode {} finished after {} time steps'.format(episode_no, time_step))\n",
    "    time_per_episode.append(time_step)\n",
    "    totaltime += time_step\n",
    "    avgtime_per_episode.append(totaltime/(episode_no+1))\n",
    "    # episode loop ends here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "Qeps = q_value_table\n",
    "\n",
    "print(Qeps)\n",
    "\n",
    "#print(evolutionQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate et exploration rate constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 10 time steps\n",
      "Episode 100 finished after 12 time steps\n",
      "Episode 200 finished after 9 time steps\n",
      "Episode 300 finished after 8 time steps\n",
      "Episode 400 finished after 11 time steps\n",
      "Episode 500 finished after 9 time steps\n",
      "Episode 600 finished after 9 time steps\n",
      "Episode 700 finished after 9 time steps\n",
      "Episode 800 finished after 10 time steps\n",
      "Episode 900 finished after 8 time steps\n",
      "Episode 1000 finished after 8 time steps\n",
      "Episode 1100 finished after 10 time steps\n",
      "Episode 1200 finished after 12 time steps\n",
      "Episode 1300 finished after 9 time steps\n",
      "Episode 1400 finished after 9 time steps\n",
      "Episode 1500 finished after 10 time steps\n",
      "Episode 1600 finished after 11 time steps\n",
      "Episode 1700 finished after 8 time steps\n",
      "Episode 1800 finished after 12 time steps\n",
      "Episode 1900 finished after 14 time steps\n",
      "Episode 2000 finished after 9 time steps\n",
      "Episode 2100 finished after 9 time steps\n",
      "Episode 2200 finished after 9 time steps\n",
      "Episode 2300 finished after 10 time steps\n",
      "Episode 2400 finished after 10 time steps\n",
      "Episode 2500 finished after 12 time steps\n",
      "Episode 2600 finished after 9 time steps\n",
      "Episode 2700 finished after 14 time steps\n",
      "Episode 2800 finished after 9 time steps\n",
      "Episode 2900 finished after 8 time steps\n",
      "Episode 3000 finished after 9 time steps\n",
      "Episode 3100 finished after 9 time steps\n",
      "Episode 3200 finished after 8 time steps\n",
      "Episode 3300 finished after 11 time steps\n",
      "Episode 3400 finished after 9 time steps\n",
      "Episode 3500 finished after 9 time steps\n",
      "Episode 3600 finished after 10 time steps\n",
      "Episode 3700 finished after 10 time steps\n",
      "Episode 3800 finished after 12 time steps\n",
      "Episode 3900 finished after 9 time steps\n",
      "Episode 4000 finished after 12 time steps\n",
      "Episode 4100 finished after 8 time steps\n",
      "Episode 4200 finished after 14 time steps\n",
      "Episode 4300 finished after 10 time steps\n",
      "Episode 4400 finished after 10 time steps\n",
      "Episode 4500 finished after 9 time steps\n",
      "Episode 4600 finished after 9 time steps\n",
      "Episode 4700 finished after 12 time steps\n",
      "Episode 4800 finished after 8 time steps\n",
      "Episode 4900 finished after 12 time steps\n",
      "Episode 5000 finished after 8 time steps\n",
      "Episode 5100 finished after 9 time steps\n",
      "Episode 5200 finished after 11 time steps\n",
      "Episode 5300 finished after 9 time steps\n",
      "Episode 5400 finished after 9 time steps\n",
      "Episode 5500 finished after 10 time steps\n",
      "Episode 5600 finished after 10 time steps\n",
      "Episode 5700 finished after 9 time steps\n",
      "Episode 5800 finished after 10 time steps\n",
      "Episode 5900 finished after 9 time steps\n",
      "Episode 6000 finished after 9 time steps\n",
      "Episode 6100 finished after 10 time steps\n",
      "Episode 6200 finished after 10 time steps\n",
      "Episode 6300 finished after 10 time steps\n",
      "Episode 6400 finished after 10 time steps\n",
      "Episode 6500 finished after 10 time steps\n",
      "Episode 6600 finished after 8 time steps\n",
      "Episode 6700 finished after 9 time steps\n",
      "Episode 6800 finished after 13 time steps\n",
      "Episode 6900 finished after 9 time steps\n",
      "Episode 7000 finished after 9 time steps\n",
      "Episode 7100 finished after 11 time steps\n",
      "Episode 7200 finished after 10 time steps\n",
      "Episode 7300 finished after 10 time steps\n",
      "Episode 7400 finished after 9 time steps\n",
      "Episode 7500 finished after 10 time steps\n",
      "Episode 7600 finished after 8 time steps\n",
      "Episode 7700 finished after 9 time steps\n",
      "Episode 7800 finished after 10 time steps\n",
      "Episode 7900 finished after 11 time steps\n",
      "Episode 8000 finished after 8 time steps\n",
      "Episode 8100 finished after 9 time steps\n",
      "Episode 8200 finished after 9 time steps\n",
      "Episode 8300 finished after 11 time steps\n",
      "Episode 8400 finished after 10 time steps\n",
      "Episode 8500 finished after 14 time steps\n",
      "Episode 8600 finished after 9 time steps\n",
      "Episode 8700 finished after 9 time steps\n",
      "Episode 8800 finished after 10 time steps\n",
      "Episode 8900 finished after 11 time steps\n",
      "Episode 9000 finished after 9 time steps\n",
      "Episode 9100 finished after 8 time steps\n",
      "Episode 9200 finished after 10 time steps\n",
      "Episode 9300 finished after 9 time steps\n",
      "Episode 9400 finished after 9 time steps\n",
      "Episode 9500 finished after 10 time steps\n",
      "Episode 9600 finished after 9 time steps\n",
      "Episode 9700 finished after 9 time steps\n",
      "Episode 9800 finished after 9 time steps\n",
      "Episode 9900 finished after 9 time steps\n",
      "Episode 10000 finished after 9 time steps\n",
      "Episode 10100 finished after 10 time steps\n",
      "Episode 10200 finished after 12 time steps\n",
      "Episode 10300 finished after 8 time steps\n",
      "Episode 10400 finished after 9 time steps\n",
      "Episode 10500 finished after 10 time steps\n",
      "Episode 10600 finished after 10 time steps\n",
      "Episode 10700 finished after 8 time steps\n",
      "Episode 10800 finished after 10 time steps\n",
      "Episode 10900 finished after 9 time steps\n",
      "Episode 11000 finished after 10 time steps\n",
      "Episode 11100 finished after 8 time steps\n",
      "Episode 11200 finished after 10 time steps\n",
      "Episode 11300 finished after 11 time steps\n",
      "Episode 11400 finished after 9 time steps\n",
      "Episode 11500 finished after 10 time steps\n",
      "Episode 11600 finished after 10 time steps\n",
      "Episode 11700 finished after 11 time steps\n",
      "Episode 11800 finished after 10 time steps\n",
      "Episode 11900 finished after 10 time steps\n",
      "Episode 12000 finished after 12 time steps\n",
      "Episode 12100 finished after 8 time steps\n",
      "Episode 12200 finished after 9 time steps\n",
      "Episode 12300 finished after 9 time steps\n",
      "Episode 12400 finished after 14 time steps\n",
      "Episode 12500 finished after 9 time steps\n",
      "Episode 12600 finished after 11 time steps\n",
      "Episode 12700 finished after 10 time steps\n",
      "Episode 12800 finished after 10 time steps\n",
      "Episode 12900 finished after 9 time steps\n",
      "Episode 13000 finished after 9 time steps\n",
      "Episode 13100 finished after 11 time steps\n",
      "Episode 13200 finished after 10 time steps\n",
      "Episode 13300 finished after 8 time steps\n",
      "Episode 13400 finished after 9 time steps\n",
      "Episode 13500 finished after 11 time steps\n",
      "Episode 13600 finished after 10 time steps\n",
      "Episode 13700 finished after 11 time steps\n",
      "Episode 13800 finished after 9 time steps\n",
      "Episode 13900 finished after 9 time steps\n",
      "Episode 14000 finished after 10 time steps\n",
      "Episode 14100 finished after 10 time steps\n",
      "Episode 14200 finished after 9 time steps\n",
      "Episode 14300 finished after 8 time steps\n",
      "Episode 14400 finished after 9 time steps\n",
      "Episode 14500 finished after 11 time steps\n",
      "Episode 14600 finished after 9 time steps\n",
      "Episode 14700 finished after 9 time steps\n",
      "Episode 14800 finished after 11 time steps\n",
      "Episode 14900 finished after 10 time steps\n",
      "Episode 15000 finished after 11 time steps\n",
      "Episode 15100 finished after 10 time steps\n",
      "Episode 15200 finished after 11 time steps\n",
      "Episode 15300 finished after 10 time steps\n",
      "Episode 15400 finished after 10 time steps\n",
      "Episode 15500 finished after 10 time steps\n",
      "Episode 15600 finished after 8 time steps\n",
      "Episode 15700 finished after 9 time steps\n",
      "Episode 15800 finished after 8 time steps\n",
      "Episode 15900 finished after 11 time steps\n",
      "Episode 16000 finished after 10 time steps\n",
      "Episode 16100 finished after 11 time steps\n",
      "Episode 16200 finished after 10 time steps\n",
      "Episode 16300 finished after 11 time steps\n",
      "Episode 16400 finished after 10 time steps\n",
      "Episode 16500 finished after 9 time steps\n",
      "Episode 16600 finished after 10 time steps\n",
      "Episode 16700 finished after 9 time steps\n",
      "Episode 16800 finished after 10 time steps\n",
      "Episode 16900 finished after 9 time steps\n",
      "Episode 17000 finished after 8 time steps\n",
      "Episode 17100 finished after 10 time steps\n",
      "Episode 17200 finished after 8 time steps\n",
      "Episode 17300 finished after 9 time steps\n",
      "Episode 17400 finished after 8 time steps\n",
      "Episode 17500 finished after 9 time steps\n",
      "Episode 17600 finished after 8 time steps\n",
      "Episode 17700 finished after 9 time steps\n",
      "Episode 17800 finished after 9 time steps\n",
      "Episode 17900 finished after 9 time steps\n",
      "Episode 18000 finished after 10 time steps\n",
      "Episode 18100 finished after 8 time steps\n",
      "Episode 18200 finished after 9 time steps\n",
      "Episode 18300 finished after 10 time steps\n",
      "Episode 18400 finished after 9 time steps\n",
      "Episode 18500 finished after 10 time steps\n",
      "Episode 18600 finished after 9 time steps\n",
      "Episode 18700 finished after 10 time steps\n",
      "Episode 18800 finished after 8 time steps\n",
      "Episode 18900 finished after 9 time steps\n",
      "Episode 19000 finished after 11 time steps\n",
      "Episode 19100 finished after 9 time steps\n",
      "Episode 19200 finished after 10 time steps\n",
      "Episode 19300 finished after 10 time steps\n",
      "Episode 19400 finished after 10 time steps\n",
      "Episode 19500 finished after 9 time steps\n",
      "Episode 19600 finished after 10 time steps\n",
      "Episode 19700 finished after 8 time steps\n",
      "Episode 19800 finished after 10 time steps\n",
      "Episode 19900 finished after 9 time steps\n",
      "[[[[[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]\n",
      "\n",
      "   [[83.43887539  2.98951607]\n",
      "    [99.99938284 99.99890893]\n",
      "    [99.99938096 99.99923536]]\n",
      "\n",
      "   [[83.89291791  8.09281976]\n",
      "    [99.99939239 99.97536172]\n",
      "    [99.9993889  99.99937496]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [99.99935748 99.99923546]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]]]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.zeros(no_buckets + (no_actions,))\n",
    "\n",
    "min_explore_rate = 0.1\n",
    "min_learning_rate = 0.07\n",
    "\n",
    "_DEBUG = False\n",
    "frames = []\n",
    "reward_per_episode = []\n",
    "time_per_episode = []\n",
    "avgtime_per_episode = []\n",
    "learning_rate_per_episode = []\n",
    "explore_rate_per_episode = []\n",
    "\n",
    "evolutionQcst = []\n",
    "\n",
    "\n",
    "\n",
    "# train the system\n",
    "totaltime = 0\n",
    "for episode_no in range(max_episodes):\n",
    "\n",
    "    explore_rate = min_explore_rate\n",
    "    learning_rate = min_learning_rate\n",
    "\n",
    "    learning_rate_per_episode.append(learning_rate)\n",
    "    explore_rate_per_episode.append(explore_rate)\n",
    "\n",
    "    # reset the environment while starting a new episode\n",
    "    observation = env.reset()\n",
    "    \n",
    "    start_state_value = bucketize_state_value(observation)\n",
    "    previous_state_value = start_state_value\n",
    "\n",
    "    #print(start_state_value)\n",
    "    #time.sleep(10)\n",
    "\n",
    "\n",
    "    done = False \n",
    "    time_step = 0\n",
    "\n",
    "    while not done:  \n",
    "        #env.render()\n",
    "        action = select_action(previous_state_value, explore_rate)\n",
    "        observation, reward_gain, done, info = env.step(action)\n",
    "        #time.sleep(1) voir ce qui se passe à chaque itération\n",
    "        state_value = bucketize_state_value(observation)\n",
    "        best_q_value = np.max(q_value_table[state_value])\n",
    "\n",
    "        #update q_value_table\n",
    "        q_value_table[previous_state_value][action] += learning_rate * (\n",
    "            reward_gain + discount * best_q_value - \n",
    "            q_value_table[previous_state_value][action])\n",
    "\n",
    "        #print(q_value_table)\n",
    "\n",
    "        evolutionQeps.append(max(q_value_table[0][0][3][1]))\n",
    "\n",
    "\n",
    "\n",
    "        previous_state_value = state_value\n",
    "\n",
    "        #Enlever le if si vous voulez plus de détails \n",
    "        \n",
    "        if episode_no % 100 == 0 and _DEBUG == True:\n",
    "            print('Episode number: {}'.format(episode_no))\n",
    "            print('Time step: {}'.format(time_step))\n",
    "            print('Previous State Value: {}'.format(previous_state_value))\n",
    "            print('Selected Action: {}'.format(action))\n",
    "            print('Current State: {}'.format(str(state_value)))\n",
    "            print('Reward Obtained: {}'.format(reward_gain))\n",
    "            print('Best Q Value: {}'.format(best_q_value))\n",
    "            print('Learning rate: {}'.format(learning_rate))\n",
    "            print('Explore rate: {}'.format(explore_rate))\n",
    "\n",
    "        \n",
    "\n",
    "        time_step += 1\n",
    "        # while loop ends here\n",
    "\n",
    "    if time_step >= solved_time:\n",
    "        no_streaks += 1\n",
    "    else:\n",
    "        no_streaks = 0\n",
    "\n",
    "    if no_streaks > streak_to_end:\n",
    "        print('CartPole problem is solved after {} episodes.'.format(episode_no))\n",
    "        break\n",
    "\n",
    "    # data log\n",
    "    if episode_no % 100 == 0:  \n",
    "        print('Episode {} finished after {} time steps'.format(episode_no, time_step))\n",
    "    time_per_episode.append(time_step)\n",
    "    totaltime += time_step\n",
    "    avgtime_per_episode.append(totaltime/(episode_no+1))\n",
    "    # episode loop ends here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "Qcst = q_value_table\n",
    "\n",
    "print(Qcst)\n",
    "\n",
    "#print(evolutionQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester notre modèle entrainé\n",
    "\n",
    "- On va suivre la meilleur décision selon notre algo et voir si ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_trained(state_value, Q):\n",
    "    \n",
    "    action = np.argmax(Q[state_value])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 finished after 200 time steps\n",
      "Test 1 finished after 200 time steps\n",
      "Test 2 finished after 200 time steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/moulpc/Documents/PAF-TO-GIT/Learning-Reinforcement-Learning/Cartpole-Qlearning.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/moulpc/Documents/PAF-TO-GIT/Learning-Reinforcement-Learning/Cartpole-Qlearning.ipynb#ch0000023?line=10'>11</a>\u001b[0m time_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/moulpc/Documents/PAF-TO-GIT/Learning-Reinforcement-Learning/Cartpole-Qlearning.ipynb#ch0000023?line=12'>13</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:  \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/moulpc/Documents/PAF-TO-GIT/Learning-Reinforcement-Learning/Cartpole-Qlearning.ipynb#ch0000023?line=14'>15</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/moulpc/Documents/PAF-TO-GIT/Learning-Reinforcement-Learning/Cartpole-Qlearning.ipynb#ch0000023?line=16'>17</a>\u001b[0m     action \u001b[39m=\u001b[39m select_action_trained(previous_state_value, Qeps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/moulpc/Documents/PAF-TO-GIT/Learning-Reinforcement-Learning/Cartpole-Qlearning.ipynb#ch0000023?line=17'>18</a>\u001b[0m     observation, reward_gain, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/core.py:233\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 233\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:188\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcarttrans\u001b[39m.\u001b[39mset_translation(cartx, carty)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoletrans\u001b[39m.\u001b[39mset_rotation(\u001b[39m-\u001b[39mx[\u001b[39m2\u001b[39m])\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviewer\u001b[39m.\u001b[39;49mrender(return_rgb_array \u001b[39m=\u001b[39;49m mode\u001b[39m==\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:114\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    112\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mreshape(buffer\u001b[39m.\u001b[39mheight, buffer\u001b[39m.\u001b[39mwidth, \u001b[39m4\u001b[39m)\n\u001b[1;32m    113\u001b[0m     arr \u001b[39m=\u001b[39m arr[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,\u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[0;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow\u001b[39m.\u001b[39;49mflip()\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monetime_geoms \u001b[39m=\u001b[39m []\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m arr \u001b[39mif\u001b[39;00m return_rgb_array \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misopen\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyglet/window/xlib/__init__.py:506\u001b[0m, in \u001b[0;36mXlibWindow.flip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m# TODO canvas.flip?\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext:\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mflip()\n\u001b[1;32m    508\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sync_resize()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyglet/gl/xlib.py:359\u001b[0m, in \u001b[0;36mXlibContext13.flip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vsync:\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_vsync()\n\u001b[0;32m--> 359\u001b[0m glx\u001b[39m.\u001b[39;49mglXSwapBuffers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_display, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglx_window)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Qaleatoire = np.zeros(no_buckets + (no_actions,))\n",
    "\n",
    "for i in range (20) :\n",
    "\n",
    "    for episode_no in range(10):\n",
    "        \n",
    "        observation = env.reset()\n",
    "        start_state_value = bucketize_state_value(observation)\n",
    "        previous_state_value = start_state_value\n",
    "        done = False \n",
    "        time_step = 0\n",
    "    \n",
    "        while not done:  \n",
    "                \n",
    "            env.render()\n",
    "\n",
    "            action = select_action_trained(previous_state_value, Qaleatoire)\n",
    "            observation, reward_gain, done, info = env.step(action)\n",
    "            #time.sleep(1) voir ce qui se passe à chaque itération\n",
    "            state_value = bucketize_state_value(observation)\n",
    "            previous_state_value = state_value\n",
    "\n",
    "                    #Enlever le if si vous voulez plus de détails \n",
    "            \n",
    "            if episode_no % 100 == 0 and _DEBUG == True:\n",
    "                print('Episode number: {}'.format(episode_no))\n",
    "                print('Time step: {}'.format(time_step))\n",
    "                print('Previous State Value: {}'.format(previous_state_value))\n",
    "                print('Selected Action: {}'.format(action))\n",
    "                print('Current State: {}'.format(str(state_value)))\n",
    "                print('Reward Obtained: {}'.format(reward_gain))\n",
    "                print('Best Q Value: {}'.format(best_q_value))\n",
    "                print('Learning rate: {}'.format(learning_rate))\n",
    "                print('Explore rate: {}'.format(explore_rate))\n",
    "\n",
    "            \n",
    "\n",
    "            time_step += 1\n",
    "            # while loop ends here\n",
    "\n",
    "        if time_step >= solved_time:\n",
    "            no_streaks += 1\n",
    "        else:\n",
    "            no_streaks = 0\n",
    "\n",
    "\n",
    "        # data log\n",
    "        if episode_no % 100 == 0:  \n",
    "            print('Test {} finished after {} time steps'.format(i, time_step))\n",
    "        time_per_episode.append(time_step)\n",
    "        totaltime += time_step\n",
    "        avgtime_per_episode.append(totaltime/(episode_no+1))\n",
    "        # episode loop ends here\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étudier l'effet de certains paramètre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le paramètre $ \\alpha $\n",
    "- On veut voir l'effet du Learning rate sur la variation de $ Q(s,a) $ pour un état donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(evolutionQeps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
